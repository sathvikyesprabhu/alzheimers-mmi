{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "from resnet import ResNet\n",
    "from unet import UNet\n",
    "from dnn import DNN\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1387755102040815 1.2606557377049181 1\n"
     ]
    }
   ],
   "source": [
    "# [245, 610, 769]\n",
    "print([769/245, 769/610, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "1\n",
      "NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1624\n",
      "[207, 483, 607] [38, 127, 162]\n"
     ]
    }
   ],
   "source": [
    "num_180 = 0\n",
    "num_166 = 0\n",
    "num_162 = 0\n",
    "num_other = 0\n",
    "\n",
    "cls_num_list_train = [0,0,0]\n",
    "cls_num_list_test = [0,0,0]\n",
    "n = 0\n",
    "bigmat = []\n",
    "\n",
    "for folder in [\"train\",\"test\"]:\n",
    "    for cls_idx,subfolder in enumerate([\"AD\",\"CN\",\"MCI\"]):\n",
    "        files = os.listdir('MRI_all/' + folder + '/' + subfolder)\n",
    "        \n",
    "        for file in files:\n",
    "            mat = np.load('MRI_all/' + folder + '/' + subfolder + '/' + file)\n",
    "            \n",
    "            if folder==\"train\":\n",
    "                bigmat.append(mat)\n",
    "                n += 1\n",
    "                cls_num_list_train[cls_idx] += 1\n",
    "            if folder==\"test\":\n",
    "                cls_num_list_test[cls_idx] +=1\n",
    "            \n",
    "            # print(folder, subfolder, file, mat.shape)\n",
    "            # if(mat.shape[0] != 166):\n",
    "            #     os.remove(folder + '/' + subfolder + '/' + file)\n",
    "            if(mat.shape[0] == 166):\n",
    "                # mat = mat[2:164]\n",
    "                num_166 += 1\n",
    "            elif(mat.shape[0] == 180):\n",
    "                # mat = mat[9:171]\n",
    "                num_180 += 1\n",
    "                # mat = mat[7:173]\n",
    "            elif(mat.shape[0] == 162):\n",
    "                num_162 += 1\n",
    "            else:\n",
    "                num_other += 1\n",
    "            # np.save(folder + '/' + subfolder + '/' + file, mat)\n",
    "            \n",
    "print(num_166, num_180, num_162)\n",
    "print(cls_num_list_train, cls_num_list_test)\n",
    "\n",
    "bigmat = np.stack(bigmat)\n",
    "print(bigmat.shape)\n",
    "\n",
    "# data = bigmat/255\n",
    "# mean_list = np.mean(data,axis=(0,2,3))\n",
    "# std_list = np.std(data,axis=(0,2,3))\n",
    "\n",
    "mean_list = np.load(\"MRI_mean.npy\")\n",
    "std_list = np.load(\"MRI_std.npy\")\n",
    "\n",
    "transform_train = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.RandomCrop(256, padding=16),\n",
    "                                     # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0, hue=0),\n",
    "                                     transforms.GaussianBlur(kernel_size = (5,5), sigma=(0.2,0.2)),\n",
    "                                     transforms.Normalize(mean_list, std_list)\n",
    "                                    ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean_list, std_list)])\n",
    "\n",
    "def npy_loader_train(path):\n",
    "    mat = np.load(path)\n",
    "    mat = np.transpose(mat, (1,2,0))\n",
    "    mat = transform_train(mat)\n",
    "    return mat\n",
    "\n",
    "def npy_loader_test(path):\n",
    "    mat = np.load(path)\n",
    "    mat = np.transpose(mat, (1,2,0))\n",
    "    mat = transform_test(mat)\n",
    "    return mat\n",
    "\n",
    "trainset_MRI = datasets.DatasetFolder(root=\"MRI_all/train\", loader=npy_loader_train, extensions=(\".npy\"))\n",
    "testset_MRI = datasets.DatasetFolder(root=\"MRI_all/test\", loader=npy_loader_test, extensions=(\".npy\"))\n",
    "\n",
    "trainloader_MRI = DataLoader(trainset_MRI, batch_size=2, shuffle=False)\n",
    "testloader_MRI = DataLoader(testset_MRI, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 52, 89] [8, 21, 35]\n",
      "(175, 100)\n"
     ]
    }
   ],
   "source": [
    "cls_num_list_train = [0,0,0]\n",
    "cls_num_list_test = [0,0,0]\n",
    "n = 0\n",
    "bigmat = []\n",
    "\n",
    "for folder in [\"train\",\"test\"]:\n",
    "    for cls_idx,subfolder in enumerate([\"AD\",\"CN\",\"MCI\"]):\n",
    "        files = os.listdir('EHR_all/' + folder + '/' + subfolder)\n",
    "        \n",
    "        for file in files:\n",
    "            mat = np.load('EHR_all/' + folder + '/' + subfolder + '/' + file)\n",
    "            \n",
    "            if folder==\"train\":\n",
    "                bigmat.append(mat)\n",
    "                n += 1\n",
    "                cls_num_list_train[cls_idx] += 1\n",
    "            if folder==\"test\":\n",
    "                cls_num_list_test[cls_idx] +=1\n",
    "            \n",
    "            # print(folder, subfolder, file, mat.shape)\n",
    "            \n",
    "print(cls_num_list_train, cls_num_list_test)\n",
    "\n",
    "bigmat = np.stack(bigmat)\n",
    "print(bigmat.shape)\n",
    "\n",
    "data = bigmat\n",
    "mean_list = np.mean(data,axis=0)\n",
    "std_list = np.std(data,axis=0) + 0.01\n",
    "\n",
    "def npy_loader_train(path):\n",
    "    mat = np.load(path)\n",
    "    # mat = transform_train(mat)\n",
    "    mat = (mat - mean_list)/std_list\n",
    "    mat = torch.Tensor(mat)\n",
    "    return mat\n",
    "\n",
    "def npy_loader_test(path):\n",
    "    mat = np.load(path)\n",
    "    # mat = transform_test(mat)\n",
    "    mat = (mat - mean_list)/std_list\n",
    "    mat = torch.Tensor(mat)\n",
    "    return mat\n",
    "\n",
    "trainset_EHR = datasets.DatasetFolder(root=\"EHR_all/train\", loader=npy_loader_train, extensions=(\".npy\"))\n",
    "testset_EHR = datasets.DatasetFolder(root=\"EHR_all/test\", loader=npy_loader_test, extensions=(\".npy\"))\n",
    "\n",
    "trainloader_EHR = DataLoader(trainset_EHR, batch_size=2, shuffle=False)\n",
    "testloader_EHR = DataLoader(testset_EHR, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (fc1): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (fc1_drop): Dropout(p=0.25, inplace=False)\n",
       "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize\n",
    "model_MRI = ResNet(in_channels= 162, num_classes=3).to(device)\n",
    "model_EHR = DNN(in_channels= 100, num_classes=3).to(device)\n",
    "\n",
    "# Load saved model weights\n",
    "model_MRI.load_state_dict(torch.load(\"checkpoint/\" + \"ResNet.pth\"))\n",
    "model_EHR.load_state_dict(torch.load(\"checkpoint/\" + \"DNN.pth\"))\n",
    "\n",
    "model_MRI.eval()\n",
    "model_EHR.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRI:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          AD     0.6667    0.0588    0.1081        34\n",
      "          CN     1.0000    0.0385    0.0741        52\n",
      "         MCI     0.5235    1.0000    0.6873        89\n",
      "\n",
      "    accuracy                         0.5314       175\n",
      "   macro avg     0.7301    0.3658    0.2898       175\n",
      "weighted avg     0.6929    0.5314    0.3925       175\n",
      "\n",
      "EHR:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          AD     0.5667    1.0000    0.7234        34\n",
      "          CN     0.7895    0.5769    0.6667        52\n",
      "         MCI     0.7792    0.6742    0.7229        89\n",
      "\n",
      "    accuracy                         0.7086       175\n",
      "   macro avg     0.7118    0.7504    0.7043       175\n",
      "weighted avg     0.7410    0.7086    0.7063       175\n",
      "\n",
      "Total:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          AD     0.5789    0.9706    0.7253        34\n",
      "          CN     1.0000    0.0385    0.0741        52\n",
      "         MCI     0.5948    0.7753    0.6732        89\n",
      "\n",
      "    accuracy                         0.5943       175\n",
      "   macro avg     0.7246    0.5948    0.4908       175\n",
      "weighted avg     0.7121    0.5943    0.5053       175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_corrects_MRI = 0; running_corrects_EHR = 0; running_corrects_total = 0;\n",
    "y = []; yhat_MRI = []; yhat_EHR = []; yhat_total = []\n",
    "\n",
    "for data_MRI, data_EHR in zip(trainloader_MRI, trainloader_EHR):\n",
    "    \n",
    "    inputs, labels = data_MRI\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs_MRI = model_MRI(inputs)\n",
    "    \n",
    "    inputs, labels = data_EHR\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs_EHR = model_EHR(inputs)\n",
    "    \n",
    "    probs_MRI = F.softmax(outputs_MRI, -1)\n",
    "    probs_EHR = F.softmax(outputs_EHR, -1)\n",
    "    entropy_MRI = get_entropy(probs_MRI)\n",
    "    entropy_EHR = get_entropy(probs_EHR)\n",
    "    \n",
    "    # Combine probability vectors\n",
    "    # probs_total = (probs_MRI + probs_EHR)/2\n",
    "    probs_total = probs_MRI/entropy_MRI + probs_EHR/entropy_EHR\n",
    "    \n",
    "    _, preds_MRI = torch.max(outputs_MRI, 1)\n",
    "    _, preds_EHR = torch.max(outputs_EHR, 1)\n",
    "    _, preds_total = torch.max(probs_total, 1)\n",
    "    \n",
    "    y.append(labels.tolist())\n",
    "    yhat_MRI.append(preds_MRI.tolist())\n",
    "    yhat_EHR.append(preds_EHR.tolist())\n",
    "    yhat_total.append(preds_total.tolist())\n",
    "    \n",
    "    running_corrects_MRI += torch.sum(preds_MRI == labels.data)\n",
    "    running_corrects_EHR += torch.sum(preds_EHR == labels.data)\n",
    "    running_corrects_total += torch.sum(preds_total == labels.data)\n",
    "\n",
    "test_accuracy_MRI = (running_corrects_MRI.float() / len(trainset_MRI))\n",
    "test_accuracy_EHR = (running_corrects_EHR.float() / len(trainset_EHR))\n",
    "test_accuracy_total = (running_corrects_total.float() / len(trainset_EHR))\n",
    "\n",
    "y = np.hstack(y)\n",
    "yhat_MRI = np.hstack(yhat_MRI)\n",
    "yhat_EHR = np.hstack(yhat_EHR)\n",
    "yhat_total = np.hstack(yhat_total)\n",
    "\n",
    "# print(\"MRI | Accuracy: {:.4f}\\n\".\n",
    "#       format(test_accuracy_MRI.item()))\n",
    "# print(\"EHR | Accuracy: {:.4f}\\n\".\n",
    "#       format(test_accuracy_EHR.item()))\n",
    "\n",
    "print(\"MRI:\\n\", classification_report(y, yhat_MRI, target_names=trainset_MRI.classes, digits=4))\n",
    "print(\"EHR:\\n \", classification_report(y, yhat_EHR, target_names=trainset_EHR.classes, digits=4))\n",
    "print(\"Total:\\n \", classification_report(y, yhat_total, target_names=trainset_EHR.classes, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRI:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          AD     0.0000    0.0000    0.0000         8\n",
      "          CN     0.0000    0.0000    0.0000        21\n",
      "         MCI     0.5469    1.0000    0.7071        35\n",
      "\n",
      "    accuracy                         0.5469        64\n",
      "   macro avg     0.1823    0.3333    0.2357        64\n",
      "weighted avg     0.2991    0.5469    0.3867        64\n",
      "\n",
      "EHR:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          AD     0.4211    1.0000    0.5926         8\n",
      "          CN     0.6667    0.5714    0.6154        21\n",
      "         MCI     0.8148    0.6286    0.7097        35\n",
      "\n",
      "    accuracy                         0.6562        64\n",
      "   macro avg     0.6342    0.7333    0.6392        64\n",
      "weighted avg     0.7170    0.6562    0.6641        64\n",
      "\n",
      "Total:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          AD     0.3889    0.8750    0.5385         8\n",
      "          CN     0.0000    0.0000    0.0000        21\n",
      "         MCI     0.6087    0.8000    0.6914        35\n",
      "\n",
      "    accuracy                         0.5469        64\n",
      "   macro avg     0.3325    0.5583    0.4099        64\n",
      "weighted avg     0.3815    0.5469    0.4454        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "running_corrects_MRI = 0; running_corrects_EHR = 0; running_corrects_total = 0;\n",
    "y = []; yhat_MRI = []; yhat_EHR = []; yhat_total = []\n",
    "\n",
    "for data_MRI, data_EHR in zip(testloader_MRI, testloader_EHR):\n",
    "    \n",
    "    inputs, labels = data_MRI\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs_MRI = model_MRI(inputs)\n",
    "    \n",
    "    inputs, labels = data_EHR\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs_EHR = model_EHR(inputs)\n",
    "    \n",
    "    probs_MRI = F.softmax(outputs_MRI, -1)\n",
    "    probs_EHR = F.softmax(outputs_EHR, -1)\n",
    "    entropy_MRI = get_entropy(probs_MRI)\n",
    "    entropy_EHR = get_entropy(probs_EHR)\n",
    "    \n",
    "    # Combine probability vectors\n",
    "    # probs_total = (probs_MRI + probs_EHR)/2\n",
    "    probs_total = probs_MRI/entropy_MRI + probs_EHR/entropy_EHR\n",
    "    \n",
    "    _, preds_MRI = torch.max(outputs_MRI, 1)\n",
    "    _, preds_EHR = torch.max(outputs_EHR, 1)\n",
    "    _, preds_total = torch.max(probs_total, 1)\n",
    "    \n",
    "    y.append(labels.tolist())\n",
    "    yhat_MRI.append(preds_MRI.tolist())\n",
    "    yhat_EHR.append(preds_EHR.tolist())\n",
    "    yhat_total.append(preds_total.tolist())\n",
    "    \n",
    "    running_corrects_MRI += torch.sum(preds_MRI == labels.data)\n",
    "    running_corrects_EHR += torch.sum(preds_EHR == labels.data)\n",
    "    running_corrects_total += torch.sum(preds_total == labels.data)\n",
    "\n",
    "test_accuracy_MRI = (running_corrects_MRI.float() / len(testset_MRI))\n",
    "test_accuracy_EHR = (running_corrects_EHR.float() / len(testset_EHR))\n",
    "test_accuracy_total = (running_corrects_total.float() / len(testset_EHR))\n",
    "\n",
    "y = np.hstack(y)\n",
    "yhat_MRI = np.hstack(yhat_MRI)\n",
    "yhat_EHR = np.hstack(yhat_EHR)\n",
    "yhat_total = np.hstack(yhat_total)\n",
    "\n",
    "# print(\"MRI | Accuracy: {:.4f}\\n\".\n",
    "#       format(test_accuracy_MRI.item()))\n",
    "# print(\"EHR | Accuracy: {:.4f}\\n\".\n",
    "#       format(test_accuracy_EHR.item()))\n",
    "\n",
    "print(\"MRI:\\n\", classification_report(y, yhat_MRI, target_names=trainset_MRI.classes, digits=4))\n",
    "print(\"EHR:\\n \", classification_report(y, yhat_EHR, target_names=trainset_EHR.classes, digits=4))\n",
    "print(\"Total:\\n \", classification_report(y, yhat_total, target_names=trainset_EHR.classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
